# Qwen大模型微调配置文件

# 模型配置
model:
  model_name_or_path: "Qwen/Qwen2-0.5B"  # 基础模型路径
  model_type: "qwen2"
  trust_remote_code: true
  torch_dtype: "bfloat16"  # 使用bfloat16精度
  attn_implementation: "eager"  # 使用标准attention实现

# 数据配置
data:
  dataset_name: "chinese_alpaca"  # 主要数据集
  additional_datasets: []
  max_seq_length: 1024
  train_split: "train"
  validation_split: "validation"
  test_split: "test"
  data_cache_dir: "./data/cache"

# 训练参数 (针对0.5B小模型优化)
training:
  output_dir: "./results/checkpoints"
  num_train_epochs: 2
  per_device_train_batch_size: 4  # 减小批次大小避免训练卡住
  per_device_eval_batch_size: 8  # 评估时使用较小批次
  gradient_accumulation_steps: 4  # 增加梯度累积保持有效批次大小
  learning_rate: 0.0002  # 小模型使用更高学习率
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  
  # 保存和评估
  save_strategy: "steps"
  save_steps: 300  # 更频繁保存，防止意外丢失
  eval_strategy: "steps"
  eval_steps: 300  # 更频繁评估，及时发现问题
  logging_steps: 50   # 更频繁日志记录
  save_total_limit: 5  # 保留更多检查点
  
  # 优化器 (针对RTX A5000优化)
  optim: "adamw_torch_fused"  # 使用融合优化器，提升性能
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0

# LoRA配置 (针对0.5B小模型优化)
lora:
  use_lora: true
  r: 8  # 小模型使用较小的LoRA秩
  lora_alpha: 16  # 相应调整alpha值
  lora_dropout: 0.1  # 适当增加dropout防止过拟合
  target_modules:
    - "q_proj"  # Qwen2的查询投影层
    - "k_proj"  # Qwen2的键投影层
    - "v_proj"  # Qwen2的值投影层
    - "o_proj"  # Qwen2的输出投影层
    - "gate_proj"  # Qwen2的门控投影层
    - "up_proj"    # Qwen2的上投影层
    - "down_proj"  # Qwen2的下投影层
  bias: "none"
  task_type: "CAUSAL_LM"

# 量化配置 (RTX A5000显存充足，可选择性使用)
quantization:
  use_4bit: false  # RTX A5000 16GB显存充足，关闭量化获得更好性能
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"

# MindSpeed配置
mindspeed:
  enable: true
  parallel_mode: "data_parallel"  # 数据并行
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  micro_batch_size: 1
  
# 监控和日志
logging:
  report_to: ["tensorboard"]  # 只使用tensorboard，避免wandb API密钥问题
  logging_dir: "./results/logs"
  run_name: "qwen_finetune_experiment"
  
# 环境配置 (针对RTX A5000优化)
environment:
  seed: 42
  dataloader_num_workers: 0  # 设置为0避免Windows下的多进程pickle错误
  dataloader_drop_last: false  # 不丢弃最后一个不完整的批次
  remove_unused_columns: false
  fp16: false
  bf16: true  # RTX A5000原生支持bfloat16
  gradient_checkpointing: false  # 显存充足时关闭，提升训练速度
  dataloader_pin_memory: true
  torch_compile: true  # 启用PyTorch编译优化
  ddp_find_unused_parameters: false  # 优化分布式训练

# 推理配置
inference:
  max_new_tokens: 512
  do_sample: true
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1