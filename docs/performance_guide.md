# æ€§èƒ½ä¼˜åŒ–ä¸è°ƒè¯•æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [æ€§èƒ½ç›‘æ§](#æ€§èƒ½ç›‘æ§)
2. [å†…å­˜ä¼˜åŒ–](#å†…å­˜ä¼˜åŒ–)
3. [è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–](#è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–)
4. [GPUåˆ©ç”¨ç‡ä¼˜åŒ–](#gpuåˆ©ç”¨ç‡ä¼˜åŒ–)
5. [æ•°æ®åŠ è½½ä¼˜åŒ–](#æ•°æ®åŠ è½½ä¼˜åŒ–)
6. [æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯](#æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯)
7. [è°ƒè¯•å·¥å…·å’ŒæŠ€å·§](#è°ƒè¯•å·¥å…·å’ŒæŠ€å·§)
8. [æ€§èƒ½åŸºå‡†æµ‹è¯•](#æ€§èƒ½åŸºå‡†æµ‹è¯•)
9. [æ•…éšœè¯Šæ–­](#æ•…éšœè¯Šæ–­)
10. [æœ€ä½³å®è·µæ€»ç»“](#æœ€ä½³å®è·µæ€»ç»“)

## ğŸ“Š æ€§èƒ½ç›‘æ§

### ç³»ç»Ÿç›‘æ§å·¥å…·

#### 1. GPUç›‘æ§

```bash
# å®æ—¶GPUç›‘æ§
watch -n 1 nvidia-smi

# è¯¦ç»†GPUä¿¡æ¯
nvidia-smi -l 1

# GPUåˆ©ç”¨ç‡å†å²
nvidia-smi --query-gpu=timestamp,name,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 1

# ä¿å­˜ç›‘æ§æ—¥å¿—
nvidia-smi --query-gpu=timestamp,utilization.gpu,memory.used --format=csv -l 1 > gpu_monitor.csv
```

#### 2. ç³»ç»Ÿèµ„æºç›‘æ§

```bash
# CPUå’Œå†…å­˜ç›‘æ§
htop

# ç£ç›˜I/Oç›‘æ§
iotop

# ç½‘ç»œç›‘æ§
iftop

# ç»¼åˆç³»ç»Ÿç›‘æ§
top -p $(pgrep -d',' python)
```

#### 3. Pythonæ€§èƒ½ç›‘æ§

```python
# å†…å­˜ä½¿ç”¨ç›‘æ§
import psutil
import GPUtil

def monitor_resources():
    # CPUä½¿ç”¨ç‡
    cpu_percent = psutil.cpu_percent(interval=1)
    
    # å†…å­˜ä½¿ç”¨
    memory = psutil.virtual_memory()
    memory_percent = memory.percent
    memory_used = memory.used / (1024**3)  # GB
    
    # GPUä½¿ç”¨ç‡
    gpus = GPUtil.getGPUs()
    if gpus:
        gpu = gpus[0]
        gpu_util = gpu.load * 100
        gpu_memory = gpu.memoryUtil * 100
        
    print(f"CPU: {cpu_percent}%, Memory: {memory_percent}% ({memory_used:.1f}GB)")
    print(f"GPU: {gpu_util}%, GPU Memory: {gpu_memory}%")
```

### è®­ç»ƒç›‘æ§

#### TensorBoardé›†æˆ

```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ è¯¦ç»†ç›‘æ§
from torch.utils.tensorboard import SummaryWriter
import time

class TrainingMonitor:
    def __init__(self, log_dir):
        self.writer = SummaryWriter(log_dir)
        self.start_time = time.time()
        
    def log_metrics(self, step, metrics):
        for key, value in metrics.items():
            self.writer.add_scalar(key, value, step)
            
    def log_system_metrics(self, step):
        # GPUç›‘æ§
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024**3
            gpu_cached = torch.cuda.memory_reserved() / 1024**3
            self.writer.add_scalar('System/GPU_Memory_GB', gpu_memory, step)
            self.writer.add_scalar('System/GPU_Cached_GB', gpu_cached, step)
            
        # è®­ç»ƒé€Ÿåº¦
        elapsed_time = time.time() - self.start_time
        samples_per_second = step / elapsed_time if elapsed_time > 0 else 0
        self.writer.add_scalar('System/Samples_Per_Second', samples_per_second, step)
```

## ğŸ’¾ å†…å­˜ä¼˜åŒ–

### 1. æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)

```yaml
# training_config.yaml
optimization:
  gradient_checkpointing: true
  
# æˆ–åœ¨ä»£ç ä¸­å¯ç”¨
model.gradient_checkpointing_enable()
```

**æ•ˆæœ**: å‡å°‘50-80%çš„GPUå†…å­˜ä½¿ç”¨ï¼Œä½†ä¼šå¢åŠ 20-30%çš„è®­ç»ƒæ—¶é—´ã€‚

### 2. æ··åˆç²¾åº¦è®­ç»ƒ

```yaml
# ä½¿ç”¨bfloat16ï¼ˆæ¨èï¼‰
training:
  bf16: true
  
# æˆ–ä½¿ç”¨float16
training:
  fp16: true
  fp16_opt_level: "O1"  # å¯é€‰: O0, O1, O2, O3
```

**å†…å­˜èŠ‚çœ**: çº¦50%çš„æ¿€æ´»å†…å­˜å’Œæ¨¡å‹å‚æ•°å†…å­˜ã€‚

### 3. LoRA (Low-Rank Adaptation)

```yaml
lora:
  use_lora: true
  r: 8                    # rankï¼Œè¶Šå°å†…å­˜è¶Šå°‘
  lora_alpha: 16
  lora_dropout: 0.1
  target_modules: 
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
```

**å†…å­˜èŠ‚çœ**: å‡å°‘90%ä»¥ä¸Šçš„å¯è®­ç»ƒå‚æ•°ã€‚

### 4. é‡åŒ–æŠ€æœ¯

```python
# 4-bité‡åŒ–
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto"
)
```

### 5. DeepSpeed ZeRO

```json
// deepspeed_config.json
{
  "zero_optimization": {
    "stage": 2,
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "contiguous_gradients": true
  },
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "initial_scale_power": 16,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 2e-5,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  },
  "scheduler": {
    "type": "WarmupLR",
    "params": {
      "warmup_min_lr": 0,
      "warmup_max_lr": 2e-5,
      "warmup_num_steps": 1000
    }
  }
}
```

### å†…å­˜ä½¿ç”¨ä¼°ç®—

```python
def estimate_memory_usage(model_size_b, sequence_length, batch_size, precision="fp16"):
    """
    ä¼°ç®—è®­ç»ƒå†…å­˜ä½¿ç”¨é‡
    
    Args:
        model_size_b: æ¨¡å‹å‚æ•°æ•°é‡ï¼ˆåäº¿ï¼‰
        sequence_length: åºåˆ—é•¿åº¦
        batch_size: æ‰¹æ¬¡å¤§å°
        precision: ç²¾åº¦ç±»å‹
    """
    # å‚æ•°å†…å­˜ (GB)
    if precision == "fp32":
        param_memory = model_size_b * 4
    elif precision in ["fp16", "bf16"]:
        param_memory = model_size_b * 2
    elif precision == "int8":
        param_memory = model_size_b * 1
    elif precision == "int4":
        param_memory = model_size_b * 0.5
    
    # æ¢¯åº¦å†…å­˜ (ä¸å‚æ•°ç›¸åŒ)
    gradient_memory = param_memory
    
    # ä¼˜åŒ–å™¨çŠ¶æ€ (Adaméœ€è¦2å€å‚æ•°å†…å­˜)
    optimizer_memory = param_memory * 2
    
    # æ¿€æ´»å†…å­˜ (è¿‘ä¼¼)
    activation_memory = batch_size * sequence_length * model_size_b * 0.001
    
    total_memory = param_memory + gradient_memory + optimizer_memory + activation_memory
    
    print(f"å‚æ•°å†…å­˜: {param_memory:.1f} GB")
    print(f"æ¢¯åº¦å†…å­˜: {gradient_memory:.1f} GB")
    print(f"ä¼˜åŒ–å™¨å†…å­˜: {optimizer_memory:.1f} GB")
    print(f"æ¿€æ´»å†…å­˜: {activation_memory:.1f} GB")
    print(f"æ€»å†…å­˜éœ€æ±‚: {total_memory:.1f} GB")
    
    return total_memory

# ç¤ºä¾‹ï¼šQwen-7Bæ¨¡å‹
estimate_memory_usage(7, 2048, 4, "bf16")
```

## âš¡ è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–

### 1. æ‰¹æ¬¡å¤§å°ä¼˜åŒ–

```python
# è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜æ‰¹æ¬¡å¤§å°
def find_optimal_batch_size(model, tokenizer, max_batch_size=32):
    for batch_size in range(1, max_batch_size + 1):
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            inputs = tokenizer(["æµ‹è¯•æ–‡æœ¬"] * batch_size, 
                             return_tensors="pt", 
                             padding=True, 
                             truncation=True)
            
            # å‰å‘ä¼ æ’­æµ‹è¯•
            with torch.no_grad():
                outputs = model(**inputs)
            
            print(f"æ‰¹æ¬¡å¤§å° {batch_size}: æˆåŠŸ")
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"æ‰¹æ¬¡å¤§å° {batch_size}: å†…å­˜ä¸è¶³")
                return batch_size - 1
            else:
                raise e
    
    return max_batch_size
```

### 2. æ¢¯åº¦ç´¯ç§¯

```yaml
# ç­‰æ•ˆå¤§æ‰¹æ¬¡è®­ç»ƒ
training:
  per_device_train_batch_size: 2      # å®é™…æ‰¹æ¬¡
  gradient_accumulation_steps: 8       # ç´¯ç§¯æ­¥æ•°
  # ç­‰æ•ˆæ‰¹æ¬¡å¤§å° = 2 * 8 * GPUæ•°é‡
```

### 3. æ•°æ®å¹¶è¡Œä¼˜åŒ–

```bash
# å¤šGPUè®­ç»ƒ
torchrun --nproc_per_node=4 \
         --master_port=29500 \
         scripts/train.py \
         --config config/training_config.yaml

# å¤šèŠ‚ç‚¹è®­ç»ƒ
torchrun --nnodes=2 \
         --nproc_per_node=4 \
         --node_rank=0 \
         --master_addr="192.168.1.100" \
         --master_port=29500 \
         scripts/train.py \
         --config config/training_config.yaml
```

### 4. ç¼–è¯‘ä¼˜åŒ–

```python
# PyTorch 2.0 ç¼–è¯‘ä¼˜åŒ–
model = torch.compile(model, mode="reduce-overhead")

# æˆ–è€…
model = torch.compile(model, mode="max-autotune")
```

### 5. æ•°æ®åŠ è½½ä¼˜åŒ–

```yaml
data:
  dataloader_num_workers: 8           # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
  dataloader_pin_memory: true         # å›ºå®šå†…å­˜
  dataloader_prefetch_factor: 2       # é¢„å–å› å­
  preprocessing_num_workers: 16       # é¢„å¤„ç†è¿›ç¨‹æ•°
```

## ğŸ¯ GPUåˆ©ç”¨ç‡ä¼˜åŒ–

### 1. GPUåˆ©ç”¨ç‡ç›‘æ§

```python
import subprocess
import time

def monitor_gpu_utilization(duration=60):
    """ç›‘æ§GPUåˆ©ç”¨ç‡"""
    start_time = time.time()
    utilizations = []
    
    while time.time() - start_time < duration:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
            capture_output=True, text=True
        )
        
        if result.returncode == 0:
            util = int(result.stdout.strip())
            utilizations.append(util)
            print(f"GPUåˆ©ç”¨ç‡: {util}%")
        
        time.sleep(1)
    
    avg_util = sum(utilizations) / len(utilizations)
    print(f"å¹³å‡GPUåˆ©ç”¨ç‡: {avg_util:.1f}%")
    return avg_util
```

### 2. æé«˜GPUåˆ©ç”¨ç‡çš„æ–¹æ³•

#### å¢åŠ æ‰¹æ¬¡å¤§å°
```yaml
training:
  per_device_train_batch_size: 8      # å°½å¯èƒ½å¤§
  gradient_accumulation_steps: 4       # ä¿æŒç­‰æ•ˆæ‰¹æ¬¡å¤§å°
```

#### ä¼˜åŒ–åºåˆ—é•¿åº¦
```yaml
data:
  max_seq_length: 2048                # æ ¹æ®GPUå†…å­˜è°ƒæ•´
  pack_sequences: true                # æ‰“åŒ…çŸ­åºåˆ—
```

#### å¼‚æ­¥æ•°æ®åŠ è½½
```python
class AsyncDataLoader:
    def __init__(self, dataloader):
        self.dataloader = dataloader
        self.stream = torch.cuda.Stream()
        
    def __iter__(self):
        first = True
        for next_batch in self.dataloader:
            with torch.cuda.stream(self.stream):
                next_batch = {k: v.cuda(non_blocking=True) 
                            for k, v in next_batch.items()}
            
            if not first:
                yield batch
            else:
                first = False
                
            torch.cuda.current_stream().wait_stream(self.stream)
            batch = next_batch
            
        yield batch
```

## ğŸ“ æ•°æ®åŠ è½½ä¼˜åŒ–

### 1. æ•°æ®é¢„å¤„ç†ä¼˜åŒ–

```python
class OptimizedDataset(torch.utils.data.Dataset):
    def __init__(self, data_path, tokenizer, max_length=2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # é¢„å…ˆåŠ è½½å’Œç¼“å­˜æ•°æ®
        self.data = self._load_and_cache_data(data_path)
        
    def _load_and_cache_data(self, data_path):
        cache_path = data_path + ".cache"
        
        if os.path.exists(cache_path):
            print("åŠ è½½ç¼“å­˜æ•°æ®...")
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        
        print("å¤„ç†åŸå§‹æ•°æ®...")
        data = []
        with open(data_path, 'r') as f:
            for line in f:
                item = json.loads(line)
                # é¢„å¤„ç†æ•°æ®
                processed_item = self._preprocess_item(item)
                data.append(processed_item)
        
        # ä¿å­˜ç¼“å­˜
        with open(cache_path, 'wb') as f:
            pickle.dump(data, f)
            
        return data
    
    def __getitem__(self, idx):
        return self.data[idx]
```

### 2. å†…å­˜æ˜ å°„æ–‡ä»¶

```python
import mmap

class MemoryMappedDataset(torch.utils.data.Dataset):
    def __init__(self, file_path):
        self.file_path = file_path
        with open(file_path, 'r') as f:
            self.line_offsets = []
            offset = 0
            for line in f:
                self.line_offsets.append(offset)
                offset += len(line.encode('utf-8'))
    
    def __len__(self):
        return len(self.line_offsets)
    
    def __getitem__(self, idx):
        with open(self.file_path, 'r') as f:
            f.seek(self.line_offsets[idx])
            line = f.readline()
            return json.loads(line)
```

### 3. å¤šè¿›ç¨‹æ•°æ®åŠ è½½

```python
from torch.utils.data import DataLoader
from torch.multiprocessing import set_start_method

# è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
set_start_method('spawn', force=True)

# ä¼˜åŒ–çš„DataLoaderé…ç½®
dataloader = DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=8,              # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
    pin_memory=True,            # å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸGPUä¼ è¾“
    prefetch_factor=2,          # é¢„å–å› å­
    persistent_workers=True,    # ä¿æŒworkerè¿›ç¨‹
    drop_last=True             # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡
)
```

## ğŸ”§ æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯

### 1. æ¨¡å‹å‰ªæ

```python
import torch.nn.utils.prune as prune

def prune_model(model, pruning_ratio=0.2):
    """ç»“æ„åŒ–å‰ªæ"""
    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Linear):
            prune.l1_unstructured(module, name='weight', amount=pruning_ratio)
            prune.remove(module, 'weight')
    
    return model

# åº”ç”¨å‰ªæ
pruned_model = prune_model(model, 0.1)  # å‰ªæ10%çš„å‚æ•°
```

### 2. çŸ¥è¯†è’¸é¦

```python
class DistillationLoss(nn.Module):
    def __init__(self, alpha=0.5, temperature=4.0):
        super().__init__()
        self.alpha = alpha
        self.temperature = temperature
        self.kl_div = nn.KLDivLoss(reduction="batchmean")
        self.ce_loss = nn.CrossEntropyLoss()
    
    def forward(self, student_logits, teacher_logits, labels):
        # è½¯æ ‡ç­¾æŸå¤±
        soft_loss = self.kl_div(
            F.log_softmax(student_logits / self.temperature, dim=-1),
            F.softmax(teacher_logits / self.temperature, dim=-1)
        ) * (self.temperature ** 2)
        
        # ç¡¬æ ‡ç­¾æŸå¤±
        hard_loss = self.ce_loss(student_logits, labels)
        
        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss
```

### 3. åŠ¨æ€æ‰¹æ¬¡å¤§å°

```python
class DynamicBatchSampler:
    def __init__(self, dataset, max_tokens=4096):
        self.dataset = dataset
        self.max_tokens = max_tokens
        
    def __iter__(self):
        batch = []
        current_tokens = 0
        
        for idx in range(len(self.dataset)):
            item_length = len(self.dataset[idx]['input_ids'])
            
            if current_tokens + item_length > self.max_tokens and batch:
                yield batch
                batch = []
                current_tokens = 0
            
            batch.append(idx)
            current_tokens += item_length
        
        if batch:
            yield batch
```

## ğŸ› ï¸ è°ƒè¯•å·¥å…·å’ŒæŠ€å·§

### 1. æ€§èƒ½åˆ†æå·¥å…·

```python
# PyTorch Profiler
from torch.profiler import profile, record_function, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    profile_memory=True,
    with_stack=True
) as prof:
    with record_function("model_training"):
        # è®­ç»ƒä»£ç 
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# ä¿å­˜åˆ†æç»“æœ
prof.export_chrome_trace("trace.json")
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

### 2. å†…å­˜æ³„æ¼æ£€æµ‹

```python
import gc
import torch

def detect_memory_leak():
    """æ£€æµ‹å†…å­˜æ³„æ¼"""
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    torch.cuda.empty_cache()
    
    # æ£€æŸ¥GPUå†…å­˜
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated()
        reserved = torch.cuda.memory_reserved()
        print(f"GPUå†…å­˜ - å·²åˆ†é…: {allocated/1024**3:.2f}GB, å·²ä¿ç•™: {reserved/1024**3:.2f}GB")
    
    # æ£€æŸ¥Pythonå¯¹è±¡
    import psutil
    process = psutil.Process()
    memory_info = process.memory_info()
    print(f"è¿›ç¨‹å†…å­˜: {memory_info.rss/1024**3:.2f}GB")

# åœ¨è®­ç»ƒå¾ªç¯ä¸­å®šæœŸè°ƒç”¨
for step, batch in enumerate(dataloader):
    # è®­ç»ƒä»£ç 
    
    if step % 100 == 0:
        detect_memory_leak()
```

### 3. æ¢¯åº¦ç›‘æ§

```python
def monitor_gradients(model):
    """ç›‘æ§æ¢¯åº¦"""
    total_norm = 0
    param_count = 0
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
            param_count += 1
            
            # æ£€æŸ¥æ¢¯åº¦å¼‚å¸¸
            if torch.isnan(param.grad).any():
                print(f"è­¦å‘Š: {name} åŒ…å«NaNæ¢¯åº¦")
            if torch.isinf(param.grad).any():
                print(f"è­¦å‘Š: {name} åŒ…å«æ— ç©·æ¢¯åº¦")
    
    total_norm = total_norm ** (1. / 2)
    print(f"æ¢¯åº¦èŒƒæ•°: {total_norm:.4f}, å‚æ•°æ•°é‡: {param_count}")
    
    return total_norm
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. è®­ç»ƒé€Ÿåº¦åŸºå‡†

```python
import time
from collections import defaultdict

class TrainingBenchmark:
    def __init__(self):
        self.metrics = defaultdict(list)
        
    def benchmark_training_step(self, model, batch, optimizer):
        """åŸºå‡†æµ‹è¯•å•ä¸ªè®­ç»ƒæ­¥éª¤"""
        torch.cuda.synchronize()
        start_time = time.time()
        
        # å‰å‘ä¼ æ’­
        forward_start = time.time()
        outputs = model(**batch)
        loss = outputs.loss
        torch.cuda.synchronize()
        forward_time = time.time() - forward_start
        
        # åå‘ä¼ æ’­
        backward_start = time.time()
        loss.backward()
        torch.cuda.synchronize()
        backward_time = time.time() - backward_start
        
        # ä¼˜åŒ–å™¨æ­¥éª¤
        optimizer_start = time.time()
        optimizer.step()
        optimizer.zero_grad()
        torch.cuda.synchronize()
        optimizer_time = time.time() - optimizer_start
        
        total_time = time.time() - start_time
        
        # è®°å½•æŒ‡æ ‡
        self.metrics['forward_time'].append(forward_time)
        self.metrics['backward_time'].append(backward_time)
        self.metrics['optimizer_time'].append(optimizer_time)
        self.metrics['total_time'].append(total_time)
        
        return {
            'forward_time': forward_time,
            'backward_time': backward_time,
            'optimizer_time': optimizer_time,
            'total_time': total_time
        }
    
    def report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        for metric, values in self.metrics.items():
            avg_time = sum(values) / len(values)
            print(f"{metric}: {avg_time:.4f}s (å¹³å‡)")
```

### 2. ååé‡æµ‹è¯•

```python
def measure_throughput(model, dataloader, num_steps=100):
    """æµ‹é‡è®­ç»ƒååé‡"""
    model.train()
    torch.cuda.synchronize()
    start_time = time.time()
    
    total_samples = 0
    for step, batch in enumerate(dataloader):
        if step >= num_steps:
            break
            
        batch_size = batch['input_ids'].size(0)
        total_samples += batch_size
        
        # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
        with torch.no_grad():
            outputs = model(**batch)
    
    torch.cuda.synchronize()
    elapsed_time = time.time() - start_time
    
    throughput = total_samples / elapsed_time
    print(f"ååé‡: {throughput:.2f} samples/second")
    print(f"æ€»æ ·æœ¬: {total_samples}, æ€»æ—¶é—´: {elapsed_time:.2f}s")
    
    return throughput
```

## ğŸ” æ•…éšœè¯Šæ–­

### å¸¸è§é—®é¢˜è¯Šæ–­æ¸…å•

#### 1. å†…å­˜é—®é¢˜

```bash
# æ£€æŸ¥GPUå†…å­˜
nvidia-smi

# æ£€æŸ¥ç³»ç»Ÿå†…å­˜
free -h

# æ£€æŸ¥è¿›ç¨‹å†…å­˜ä½¿ç”¨
ps aux | grep python
```

**è§£å†³æ–¹æ¡ˆ**:
- å‡å°‘æ‰¹æ¬¡å¤§å°
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä½¿ç”¨LoRAæˆ–é‡åŒ–
- å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°

#### 2. è®­ç»ƒé€Ÿåº¦æ…¢

```python
# æ£€æŸ¥GPUåˆ©ç”¨ç‡
def check_gpu_utilization():
    result = subprocess.run(
        ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
        capture_output=True, text=True
    )
    return int(result.stdout.strip())

util = check_gpu_utilization()
if util < 80:
    print("GPUåˆ©ç”¨ç‡ä½ï¼Œå¯èƒ½çš„åŸå› :")
    print("- æ‰¹æ¬¡å¤§å°å¤ªå°")
    print("- æ•°æ®åŠ è½½ç“¶é¢ˆ")
    print("- CPUé¢„å¤„ç†ç“¶é¢ˆ")
```

#### 3. æ¢¯åº¦é—®é¢˜

```python
def diagnose_gradients(model):
    """è¯Šæ–­æ¢¯åº¦é—®é¢˜"""
    has_nan = False
    has_inf = False
    zero_grad_params = 0
    total_params = 0
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            total_params += 1
            
            if torch.isnan(param.grad).any():
                has_nan = True
                print(f"NaNæ¢¯åº¦: {name}")
            
            if torch.isinf(param.grad).any():
                has_inf = True
                print(f"æ— ç©·æ¢¯åº¦: {name}")
            
            if param.grad.abs().max() < 1e-8:
                zero_grad_params += 1
    
    print(f"æ¢¯åº¦è¯Šæ–­ç»“æœ:")
    print(f"- åŒ…å«NaN: {has_nan}")
    print(f"- åŒ…å«æ— ç©·: {has_inf}")
    print(f"- é›¶æ¢¯åº¦å‚æ•°: {zero_grad_params}/{total_params}")
```

## ğŸ† æœ€ä½³å®è·µæ€»ç»“

### 1. å†…å­˜ä¼˜åŒ–ä¼˜å…ˆçº§

1. **å¯ç”¨LoRA** - æœ€å¤§çš„å†…å­˜èŠ‚çœ
2. **ä½¿ç”¨æ··åˆç²¾åº¦** - 50%å†…å­˜èŠ‚çœ
3. **æ¢¯åº¦æ£€æŸ¥ç‚¹** - å¤§å¹…å‡å°‘æ¿€æ´»å†…å­˜
4. **ä¼˜åŒ–æ‰¹æ¬¡å¤§å°** - å¹³è¡¡å†…å­˜å’Œé€Ÿåº¦
5. **é‡åŒ–æŠ€æœ¯** - è¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹

### 2. é€Ÿåº¦ä¼˜åŒ–ä¼˜å…ˆçº§

1. **å¢åŠ æ‰¹æ¬¡å¤§å°** - æé«˜GPUåˆ©ç”¨ç‡
2. **å¤šGPUè®­ç»ƒ** - çº¿æ€§åŠ é€Ÿ
3. **ä¼˜åŒ–æ•°æ®åŠ è½½** - æ¶ˆé™¤I/Oç“¶é¢ˆ
4. **ç¼–è¯‘ä¼˜åŒ–** - PyTorch 2.0ç¼–è¯‘
5. **å¼‚æ­¥å¤„ç†** - é‡å è®¡ç®—å’Œé€šä¿¡

### 3. ç›‘æ§æŒ‡æ ‡

- **GPUåˆ©ç”¨ç‡**: ç›®æ ‡ >85%
- **GPUå†…å­˜ä½¿ç”¨**: ç›®æ ‡ >90%
- **è®­ç»ƒé€Ÿåº¦**: samples/second
- **æ¢¯åº¦èŒƒæ•°**: æ£€æµ‹æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
- **æŸå¤±æ›²çº¿**: ç›‘æ§è®­ç»ƒè¿›åº¦

### 4. è°ƒè¯•æµç¨‹

1. **åŸºçº¿æµ‹è¯•** - è®°å½•åˆå§‹æ€§èƒ½
2. **é€æ­¥ä¼˜åŒ–** - ä¸€æ¬¡æ”¹å˜ä¸€ä¸ªå‚æ•°
3. **æ€§èƒ½ç›‘æ§** - æŒç»­ç›‘æ§å…³é”®æŒ‡æ ‡
4. **é—®é¢˜å®šä½** - ä½¿ç”¨profilingå·¥å…·
5. **éªŒè¯æ”¹è¿›** - ç¡®è®¤ä¼˜åŒ–æ•ˆæœ

---

**è®°ä½**: æ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œéœ€è¦æ ¹æ®å…·ä½“ç¡¬ä»¶å’Œä»»åŠ¡ç‰¹ç‚¹è¿›è¡Œè°ƒæ•´ã€‚å§‹ç»ˆåœ¨ä¼˜åŒ–å‰åè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç¡®ä¿æ”¹è¿›çš„æœ‰æ•ˆæ€§ã€‚